---
title: "Apache Camel × AI ― サービングによる推論 #1: TorchServe"
emoji: "🐪"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["ai", "機械学習", "tensorflow", "camel", "mlops"]
published: true
---
![Camel AI](https://storage.googleapis.com/zenn-user-upload/2f89c845b33c-20250210.jpg =300x)

[前回の記事](https://zenn.dev/tadayosi/articles/69933eb96d6f68)でも紹介した通り、先日リリースされた[Apache Camel 4.10 LTS](https://camel.apache.org/blog/2025/02/RELEASE-4.10.0/)ではAIモデルサービングに関する3つの新しいコンポーネントが追加されました。[^1]

[^1]: Camel TorchServeコンポーネントは4.9から。

* [TorchServeコンポーネント](https://camel.apache.org/components/next/torchserve-component.html)
* [TensorFlow Servingコンポーネント](https://camel.apache.org/components/next/tensorflow-serving-component.html)
* [KServeコンポーネント](https://camel.apache.org/components/next/kserve-component.html)

[前回](https://zenn.dev/tadayosi/articles/69933eb96d6f68)はTorchServeコンポーネントについて書いたので、今回はTensorFlow Servingコンポーネントについて紹介します。

## TensorFlow Servingコンポーネント

[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)は機械学習フレームワークのTensorFlowが提供するサービング機能です。[Camel TensorFlow Serving](https://camel.apache.org/components/next/tensorflow-serving-component.html)コンポーネントを使うことで、TensorFlow Servingの[gRPC Client APIs](https://github.com/tensorflow/serving/blob/2.18.0/tensorflow_serving/apis/prediction_service.proto)を通してTensorFlowモデルサーバーにデプロイされているAIモデルを呼び出せるようになります。

## 準備

改めて、まだ[Camel CLI](https://camel.apache.org/manual/camel-jbang.html)がインストールされていなければインストールしてください。

:::message
JBangがインストールされていない場合は、まずこちらを参考にJBangをインストールしてください。
https://www.jbang.dev/download/
:::

```console
jbang app install camel@apache/camel
```

インストールできたか動作確認をします。

```console
$ camel --version
4.10.0
```

### モデルの準備とサーバー立ち上げ

次に、TensorFlow Servingのサーバーを立ち上げます。最も手軽な方法は[Dockerイメージを使う方法](https://www.tensorflow.org/tfx/serving/docker)ですが、記事執筆時点では公式のDockerイメージ`tensorflow/serving`は`amd64`アーキテクチャにしか対応していないため、macOSユーザーでも試せるように`arm64`にも対応している[BitnamiのDockerイメージ](https://github.com/bitnami/containers/tree/main/bitnami/tensorflow-serving)を使うことにします。

PyTorchと違い、TensorFlow Servingではサーバー起動時にロードするモデルを予め指定する必要があります。本記事では2つのモデルを使います。

- `half_plus_two`（`x/2 + 2`） ⋯ TensorFlow Servingのリポジトリにあるテスト用モデル（[testdata](https://github.com/tensorflow/serving/tree/master/tensorflow_serving/servables/tensorflow/testdata)）
- `mnist` ⋯ TensorFlowで予め学習したMNISTのモデル

簡略化のためにサンプルコードのリポジトリにそのまま使えるモデル [models](https://github.com/megacamelus/camel-ai-examples/tree/main/tensorflow-serving/models) を用意していますので、こちらを直接ダウンロードしてください。

:::message
GitHubリポジトリの特定ディレクトリ以下のファイルをまとめてダウンロードするには、リポジトリを丸ごとクローンしてもいいですが、[VS Code for the Web](https://code.visualstudio.com/docs/editor/vscode-web)を使うのが最も簡単です。GitHubリポジトリを表示した状態でキーボードの「`.`」を押すかURLを`github.com`から`github.dev`に書き換えることで、そのリポジトリをブラウザ上のVS Codeで開けます。その後、ダウンロードしたいディレクトリを見つけて右クリックメニューから「ダウンロード」を選択します。新規に作成したディレクトリを選択することで、そのディレクトリごとファイルを一括ダウンロードできます。
:::

`models`がダウンロードできたら、`models`ディレクトリがある場所から以下のコマンドでコンテナーを起動します。

```console
docker run --rm -it --name tf-serving \
    -p 8500:8500 -p 8501:8501 \
    -v ./models:/models \
    -v ./models/models.pbtxt:/bitnami/tensorflow-serving/conf/tensorflow-serving.conf \
    bitnami/tensorflow-serving
```

## モデルの操作

:::message
Camel上でTensorFlowモデルを操作する方法に興味がない方や、推論の方法だけを手早く知りたい方は、ここを飛ばして[推論](#推論)のセクションから読んでも結構です。
:::

TensorFlow Servingが提供するモデル管理系の操作は基本的に2つです。

- ステータスチェック（[Model status API](https://www.tensorflow.org/tfx/serving/api_rest#model_status_api)）
- メタデータ取得（[Model Metadata API](https://www.tensorflow.org/tfx/serving/api_rest#model_metadata_api)）

それぞれCamelルートからどうやって呼び出せるかを見ていきます。

### モデルのステータスチェック

まず、モデルが推論可能な状態にあるかどうかを次のエンドポイントで確認できます。MNISTモデルのステータスを確認してみます。

```uri
tensorflow-serving:model-status?modelName=mnist&modelVersion=1
```

```java:model_status.java
//DEPS org.apache.camel:camel-bom:4.10.0@pom
//DEPS org.apache.camel:camel-core
//DEPS org.apache.camel:camel-tensorflow-serving

import org.apache.camel.builder.RouteBuilder;

public class model_status extends RouteBuilder {
    @Override
    public void configure() throws Exception {
        from("timer:model-status?repeatCount=1")
            .to("tensorflow-serving:model-status?modelName=mnist&modelVersion=1")
            .log("Status: ${body.getModelVersionStatus(0).state}");
    }
}
```

Camel CLIから以下のように実行します。

```console
camel run model_status.java
```

成功すれば、以下のようにMNISTモデルのステータスを確認できます。

```console
Status: AVAILABLE
```

### モデルのメタデータ取得

TensorFlowのモデルを呼び出して推論するには、モデルの入出力のシグネチャーを確認することが重要です。

:::message
TensorFlowでは、入出力のテンソルの形式を正確に合わせないと、データをモデルに渡すことも結果を受け取ることもできません。
:::

通常は、以下の[REST API](https://www.tensorflow.org/tfx/serving/api_rest#model_metadata_api)を呼び出し、JSON形式でモデルシグネチャーを確認します。（`mnist`モデルの場合）

<http://localhost:8501/v1/models/mnist/metadata>

REST APIを叩いて確認するのに比べると有用性は少ないと思われますが、Camelルートからもモデルのメタデータを取得可能です。

```uri
tensorflow-serving:model-metadata?modelName=mnist&modelVersion=1
```

```java:model_metadata.java
//DEPS org.apache.camel:camel-bom:4.10.0@pom
//DEPS org.apache.camel:camel-core
//DEPS org.apache.camel:camel-tensorflow-serving

import org.apache.camel.builder.RouteBuilder;

public class model_metadata extends RouteBuilder {
    @Override
    public void configure() throws Exception {
        from("timer:model-metadata?repeatCount=1")
            .to("tensorflow-serving:model-metadata?modelName=mnist&modelVersion=1")
            .log("Metadata: ${body.getMetadataOrThrow('signature_def')}");
    }
}
```

Camel CLIから以下のように実行します。

```console
camel run model_metadata.java
```

成功すれば、以下のようにMNISTモデルのメタデータが得られていることが確認できます。

```console
Metadata: type_url: "type.googleapis.com/tensorflow.serving.SignatureDefMap"
value: "\n\245\001\n\005serve\022\233\001\n?\n\fkeras_tensor\022/\n\024serve_keras_tensor:0\020\001\032\025\022\v\b\377\377\377\377\377\377\377\377\377\001\022\002\b\034\022\002\b\034\022<\n\boutput_0\0220\n\031StatefulPartitionedCall:0\020\001\032\021\022\v\b\377\377\377\377\377\377\377\377\377\001\022\002\b\n\032\032tensorflow/serving/predict\n\273\001\n\017serving_default\022\247\001\nI\n\fkeras_tensor\0229\n\036serving_default_keras_tensor:0\020\001\032\025\022\v\b\377\377\377\377\377\377\377\377\377\001\022\002\b\034\022\002\b\034\022>\n\boutput_0\0222\n\033StatefulPartitionedCall_1:0\020\001\032\021\022\v\b\377\377\377\377\377\377\377\377\377\001\022\002\b\n\032\032tensorflow/serving/predict\n>\n\025__saved_model_init_op\022%\022#\n\025__saved_model_init_op\022\n\n\004NoOp\032\002\030\001"
```

## 推論

本題の推論です。実際のCamelルートで主に使うのはこの操作（エンドポイント）です。TensorFlow Servingには、次の3種類の推論APIが用意されています。

- [Predict API](https://www.tensorflow.org/tfx/serving/api_rest#predict_api) ⋯ 汎用の推論API
- [Classify API](https://www.tensorflow.org/tfx/serving/api_rest#classify_and_regress_api) ⋯ 分類問題に特化した推論API
- [Regress API](https://www.tensorflow.org/tfx/serving/api_rest#classify_and_regress_api) ⋯ 回帰分析に特化した推論API

### 汎用の推論（Predict）

まずは汎用のPredict APIから見ていきます。このAPIは特定の問題によらず、どんなモデルでも呼び出せます。その代わり、データ入出力のAPIも汎用になっており、データを適切に変換するまでが煩雑です。

最初にデプロイしたMNISTモデルをこのPredict APIから呼び出してみます。MNISTは、28x28のグレースケールの手書きの画像を数字として認識させるモデルです。TorchServeのときに使った同じ[テストデータ](https://github.com/pytorch/serve/tree/master/examples/image_classifier/mnist/test_data)を今回も使います。

![MNISTによる手書き数字の認識](https://storage.googleapis.com/zenn-user-upload/a2fe9a20782c-20250219.png)
*MNISTによる手書き数字の認識*

推論には以下のエンドポイントを使います。

```uri
tensorflow-serving:predict?modelName=mnist&modelVersion=1
```

```java:predict.java
//DEPS org.apache.camel:camel-bom:4.10.0@pom
//DEPS org.apache.camel:camel-core
//DEPS org.apache.camel:camel-tensorflow-serving

import java.awt.image.BufferedImage;
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.stream.IntStream;
import javax.imageio.ImageIO;
import org.apache.camel.Exchange;
import org.apache.camel.RuntimeCamelException;
import org.apache.camel.builder.RouteBuilder;
import org.tensorflow.framework.DataType;
import org.tensorflow.framework.TensorProto;
import org.tensorflow.framework.TensorShapeProto;
import org.tensorflow.framework.TensorShapeProto.Dim;
import com.google.protobuf.Int64Value;
import tensorflow.serving.Model.ModelSpec;
import tensorflow.serving.Predict.PredictRequest;
import tensorflow.serving.Predict.PredictResponse;

public class predict extends RouteBuilder {
    @Override
    public void configure() throws Exception {
        from("file:data?noop=true&recursive=true&include=.*\\.png")
            .process(this::toPredictRequest)   // (1)
            .to("tensorflow-serving:predict?modelName=mnist&modelVersion=1")
            .process(this::argmax)             // (2)
            .log("${headers.camelFileName} => ${body}");
    }

    void toPredictRequest(Exchange exchange) { // (3)
        byte[] body = exchange.getMessage().getBody(byte[].class);
        List<Float> data = preprocess(body);
        TensorProto inputs = TensorProto.newBuilder()
                .setDtype(DataType.DT_FLOAT)
                .setTensorShape(TensorShapeProto.newBuilder()
                        .addDim(Dim.newBuilder().setSize(28))
                        .addDim(Dim.newBuilder().setSize(28)))
                .addAllFloatVal(data)
                .build();
        PredictRequest request = PredictRequest.newBuilder()
                .putInputs("keras_tensor", inputs)
                .build();
        exchange.getMessage().setBody(request);
    }

    List<Float> preprocess(byte[] data) {      // (4)
        try {
            BufferedImage image = ImageIO.read(new ByteArrayInputStream(data));
            int width = image.getWidth();
            int height = image.getHeight();
            if (width != 28 || height != 28) {
                throw new RuntimeCamelException("Image size must be 28x28");
            }
            List<Float> normalised = new ArrayList<>(width * height);
            for (int y = 0; y < height; y++) {
                for (int x = 0; x < width; x++) {
                    int rgb = image.getRGB(x, y);
                    normalised.add((rgb & 0xFF) / 255.0f);
                }
            }
            return normalised;
        } catch (IOException e) {
            throw new RuntimeCamelException("Error reading image", e);
        }
    }

    void argmax(Exchange exchange) {           // (5)
        PredictResponse response = exchange.getMessage().getBody(PredictResponse.class);
        TensorProto tensor = response.getOutputsOrThrow("output_0");
        int result = IntStream.range(0, tensor.getFloatValCount())
                .reduce((max, i) -> tensor.getFloatVal(max) > tensor.getFloatVal(i) ? max : i)
                .orElseThrow();
        exchange.getMessage().setBody(result);
    }
}
```

コードの要点を解説します。

1. 推論エンドポイントを呼び出すには、入力データをTensorFlow Servingの`PredictRequest`オブジェクトに変換する必要があります。
2. 推論エンドポイントから返ってきた`PredictResponse`オブジェクトから、後続処理のために出力データを取り出します。ここではArgmax関数を使います。
3. ファイルの`byte[]`データを`PredictRequest`に変換します。まずデータを適切に前処理（`preprocess(body)`）し、データ型（`DT_FLOAT`）と次元（`28x28`）の一致した`TensorProto`オブジェクトでラップします。最後にその`TensorProto`入力オブジェクトをラベル`keras_tensor`で`PredictRequest`に登録します。TensorFlow Servingでモデルを呼び出すにはこれらのパラメーターをすべて正確に設定する必要がありますが、これらの情報はすべてモデルのメタデータから取得できます（[モデルのメタデータ取得](#モデルのメタデータ取得)）。
4. 画像ファイルを読み込んだ`byte[]`データの前処理です。ここでは、28x28のRGB画像データから青色だけを抜き出し、MNISTモデルが期待する0〜1のFloat値に正規化しています。
5. Argmax関数を提供する手軽なライブラリはJavaにはないので、自分で実装します。`PredictResponse`から出力データを取得するときのラベル`output_0`も、モデルのメタデータから取得できます（[モデルのメタデータ取得](#モデルのメタデータ取得)）。

さて、コードを実行するには、[テストデータ](https://github.com/pytorch/serve/tree/master/examples/image_classifier/mnist/test_data)をローカルの`data/`ディレクトリにダウンロードした後、Camel CLIから以下のようにします。

```console
camel run predict.java
```

成功すれば、以下のような結果が得られるでしょう。手書きの数字が正しく認識されていることが確認できます。

```console
8.png => 8
9.png => 9
4.png => 4
5.png => 5
7.png => 7
6.png => 6
2.png => 2
3.png => 3
1.png => 1
0.png => 0
```

### 分類（Classify）

次にClassify APIを紹介します。このAPIは、入力データ群を特定のカテゴリーに分類する分類問題を実行するためのものです。サンプルデータのリストを入力として受け取り、各分類ラベルにスコアを付けたリストを出力として返します。

先ほどのMNISTモデルはClassify APIに対応していないので、代わりにもう1つのデプロイしたモデル`half_plus_two`を用います。このサンプルモデルは非常に簡単なモデルで、名前から分かるように単に入力値 $x$ に対して、

$$
\frac{x}{2} + 2
$$

を計算して返すだけです。答えはラベルのない単一のスコアとして返されます。

![half_plus_two](https://storage.googleapis.com/zenn-user-upload/a972277ebd37-20250219.png)
*half_plus_two*

推論には以下のエンドポイントを使います。このモデルは複数のシグネチャを持っているので、エンドポイントオプション`signatureName`でシグネチャ`classify_x_to_y`を指定する必要があります。

```uri
tensorflow-serving:classify?modelName=half_plus_two&modelVersion=123&signatureName=classify_x_to_y
```

```java:classify.java
//DEPS org.apache.camel:camel-bom:4.10.0@pom
//DEPS org.apache.camel:camel-core
//DEPS org.apache.camel:camel-tensorflow-serving

import org.apache.camel.builder.RouteBuilder;
import org.tensorflow.example.Example;
import org.tensorflow.example.Feature;
import org.tensorflow.example.Features;
import org.tensorflow.example.FloatList;
import tensorflow.serving.InputOuterClass.ExampleList;
import tensorflow.serving.InputOuterClass.Input;

public class classify extends RouteBuilder {
    @Override
    public void configure() throws Exception {
        from("timer:classify?repeatCount=1")
            .setBody(constant(createInput("x", 1.0f)))
            .to("tensorflow-serving:classify?modelName=half_plus_two&modelVersion=123&signatureName=classify_x_to_y")
            .log("Result: ${body.result}");
    }

    Input createInput(String key, float f) {  // (1)
        Feature feature = Feature.newBuilder()
                .setFloatList(FloatList.newBuilder().addValue(f))
                .build();
        Features features = Features.newBuilder()
                .putFeature(key, feature)
                .build();
        Example example = Example.newBuilder()
                .setFeatures(features)
                .build();
        ExampleList exampleList = ExampleList.newBuilder()
                .addExamples(example)
                .build();
        return Input.newBuilder()
                .setExampleList(exampleList)
                .build();
    }
}
```

Predict APIの時と同様、入力データとして`Input`オブジェクトを作る必要がありますが、

1. 1つひとつの特徴量（`Feature`）をまとめた（`Features`）サンプル（`Example`）をさらにリスト（`ExampleList`）にまとめる

だけなので、Predict APIに比べて入力データの作成が簡単です。

Camel CLIから以下のように実行します。

```console
camel run classify.java
```

成功すれば、以下のような結果が得られるでしょう。$\frac{1.0}{2} + 2 = 2.5$です。

```console
Result: classifications {
  classes {
    score: 2.5
  }
}
```

### 回帰分析（Regress）

最後にRegress APIです。このAPIは、入力データの系列からそれらの関係性を予測する手法、回帰分析を実行するためのものです。サンプルデータのリストを入力として受け取り、予測されたデータ系列のリストを出力として返します。

先ほどと同様、`half_plus_two`モデルを用います。

推論には以下のエンドポイントを使います。エンドポイントオプション`signatureName`でシグネチャ`regress_x_to_y`を指定します。

```uri
tensorflow-serving:regress?modelName=half_plus_two&modelVersion=123&signatureName=regress_x_to_y
```

```java:regress.java
//DEPS org.apache.camel:camel-bom:4.10.0@pom
//DEPS org.apache.camel:camel-core
//DEPS org.apache.camel:camel-tensorflow-serving

import org.apache.camel.builder.RouteBuilder;
import org.tensorflow.example.Example;
import org.tensorflow.example.Feature;
import org.tensorflow.example.Features;
import org.tensorflow.example.FloatList;
import tensorflow.serving.InputOuterClass.ExampleList;
import tensorflow.serving.InputOuterClass.Input;

public class regress extends RouteBuilder {
    @Override
    public void configure() throws Exception {
        from("timer:regress?repeatCount=1")
            .setBody(constant(createInput("x", 1.0f)))
            .to("tensorflow-serving:regress?modelName=half_plus_two&modelVersion=123&signatureName=regress_x_to_y")
            .log("Result: ${body.result}");
    }

    Input createInput(String key, float f) {
        Feature feature = Feature.newBuilder()
                .setFloatList(FloatList.newBuilder().addValue(f))
                .build();
        Features features = Features.newBuilder()
                .putFeature(key, feature)
                .build();
        Example example = Example.newBuilder()
                .setFeatures(features)
                .build();
        ExampleList exampleList = ExampleList.newBuilder()
                .addExamples(example)
                .build();
        return Input.newBuilder()
                .setExampleList(exampleList)
                .build();
    }
}
```

Camel CLIから以下のように実行します。

```console
camel run regress.java
```

成功すれば、以下のような結果が得られるでしょう。通常は値のリストが返りますが、このモデルでは1つだけです。$\frac{1.0}{2} + 2 = 2.5$です。

```console
Result: regressions {
  value: 2.5
}
```

## まとめ

TorchServeコンポーネントに引き続き、最新のCamel 4.10.0 LTSリリースで使えるAIモデルサービングコンポーネントの1つ、TensorFlow Servingコンポーネントの機能を一通り見てきました。

TensorFlow Servingコンポーネントを使えば、Camelをベースに構築したインテグレーションにTensorFlowで学習したAIモデルを簡単に取り入れられるようになります。TensorFlowベースの創造的なAIインテグレーションシステムの可能性が広がります。

次回は、最後にKServeコンポーネントを紹介します。

### サンプルコード

今回紹介したCamel×AIのサンプルコードは、このリポジトリで公開しています。

https://github.com/megacamelus/camel-ai-examples
